\capitulo{4}{Técnicas y herramientas}

\subsection{Entorno de desarrollo}

Se ha trabajado en un sistema operativo \textit{Ubuntu} y utilizando \textit{Visual Studio Code} 
(VSCode) como entorno de desarrollo integrado (IDE) preferido\footnote{ La elección del sistema 
operativo y el IDE depende de las preferencias personales del autor}. VSCode se destaca por 
su versatilidad y extensibilidad, lo que me permite personalizarlo y adaptarlo a mis necesidades 
específicas. Su amplia selección de extensiones me brinda herramientas adicionales y funcionalidades 
especializadas que enriquecen mi experiencia de programación.

El lenguaje de programación elegido es \textit{Python}, reconocido por su facilidad de uso y amplia 
gama de bibliotecas y frameworks disponibles\footnote{ Python se ha utilizado previamente en el 
Trabajo Final de Grado anterior}. Sin embargo, también considero esencial el uso de \textit{Bash}, 
un intérprete de comandos de Unix, para realizar diversas tareas y automatizaciones en el entorno 
de desarrollo.

Para llevar a cabo mis proyectos, cuento con un ordenador portátil de gama media equipado con un p
rocesador \textit{Intel® Core™ i5-11400H} y 16 GB de RAM. Estas especificaciones brindan un rendimiento 
adecuado para el desarrollo y la ejecución del software que acostumbro a usar\footnote{ Aunque el 
rendimiento puede variar dependiendo de los requisitos específicos del proyecto}. Sin embargo, en 
algunos casos ha habido incidentes debido al elevado consumo de memoria que requiere el procesamiento 
de la masiva cantidad de datos a la que nos hemos enfrentado.

A continuación se presenta una lista de los paquetes\footnote{Paquetes de Python} utilizados, destacando sus funcionalidades:

\begin{itemize}
  \item \textit{Pandas}: Una biblioteca de análisis de datos de alto rendimiento que proporciona 
  estructuras de datos y herramientas para manipular y analizar conjuntos de datos complejos.
  \item \textit{tqdm}: Una biblioteca que agrega una barra de progreso elegante y visual a los 
  bucles iterativos, lo que facilita el seguimiento del progreso de las operaciones en tiempo real.
  \item \textit{Requests}: Una biblioteca que simplifica el manejo de solicitudes HTTP, 
  permitiéndome realizar peticiones a servidores web y recibir respuestas de manera sencilla.
  \item \textit{BeautifulSoup4}: Una biblioteca que se utiliza para extraer información de páginas web 
  y realizar el análisis de datos web. Facilita la extracción de datos estructurados y no estructurados 
  mediante técnicas de web scraping.
  \item \textit{Selenium}: Una biblioteca que automatiza la interacción con navegadores web, lo que 
  me permite realizar pruebas de aplicaciones web o realizar acciones específicas en páginas web de 
  forma programática.
  \item \textit{Networkx}: Una biblioteca para el análisis de redes y grafos. Proporciona herramientas 
  para la creación, manipulación y estudio de estructuras de redes complejas.
  \item \textit{Matplotlib}: Una biblioteca de visualización de datos en 2D que me permite crear 
  gráficos y visualizaciones de datos de alta calidad.
  \item \textit{Pybraries}: Una biblioteca que hace de wrapper de el API de 
  \textit{libraries.io}\footnote{Libraries.io es una plataforma en línea que proporciona información 
  y datos sobre diferentes bibliotecas y paquetes de software de código abierto} para python.
  \item \textit{Typing\_extensions}: Una extensión del módulo typing de Python que proporciona 
  funcionalidades adicionales para anotaciones de tipos en tiempo de ejecución.
  \item \textit{pdoc}: Una biblioteca que me permite generar documentación automática a partir 
  de mis archivos de código fuente.
\end{itemize}

\subsection{Jupyter Notebooks y Computación en la nube}

\textit{Jupyter Notebooks} se ha convertido en una herramienta fundamental en el ámbito de la ciencia de datos y 
la programación interactiva. Estos notebooks permiten combinar código, texto explicativo y resultados 
visuales en un solo documento, lo que facilita la comunicación y colaboración en proyectos de análisis 
de datos. Los notebooks se ejecutan en un entorno interactivo, lo que permite explorar y experimentar 
con el código de manera iterativa, lo que resulta especialmente útil en tareas de análisis exploratorio 
de datos.\footnote{El análisis exploratorio de datos implica investigar y comprender los datos a través 
de la exploración visual, la estadística descriptiva y otras técnicas para obtener ideas y patrones 
clave.}

En cuanto a la computación en la nube, ha desempeñado un papel clave en el desarrollo de este TFG.
Plataformas como \textit{Kaggle} o \textit{Deepnote} proporcionan servicios de notebooks basados en la nube, lo que 
significa que los usuarios pueden acceder a un entorno de desarrollo completo sin tener que preocuparse 
por configurar y mantener su propia infraestructura. Esto es especialmente beneficioso en proyectos 
que requieren una gran cantidad de recursos computacionales, como el procesamiento de grandes volúmenes
 de datos.\footnote{El procesamiento de grandes volúmenes de datos implica trabajar con conjuntos de 
 datos masivos que pueden superar la capacidad de procesamiento de una sola máquina. La computación 
 en la nube permite distribuir y escalar el procesamiento para manejar estos volúmenes de datos.}

Además, la computación en la nube ha ayudado a reducir los costos asociados con la obtención y 
procesamiento de datos. La obtención de datos puede requerir tiempo, memoria y almacenamiento 
significativos, lo que puede ser costoso en términos de recursos locales. Al aprovechar la computación 
en la nube podemos acceder a recursos escalables y flexibles según 
sea necesario, lo que nos permite realizar análisis más eficientes y a gran escala sin incurrir en 
costos excesivos.\footnote{La escalabilidad y flexibilidad de los recursos en la computación en la 
nube se refiere a la capacidad de aumentar o disminuir la capacidad de cómputo y almacenamiento 
según las necesidades del proyecto, lo que permite un uso más eficiente de los recursos y un mejor 
control de costos.}

\subsection{Sistema de control de versiones}

\textit{GitHub} ha desempeñado un papel fundamental como plataforma de control de versiones Git en el ámbito del 
desarrollo de software colaborativo de código abierto. Como un estándar reconocido internacionalmente, 
GitHub, adquirido por \textit{Microsoft}, ha proporcionado a los desarrolladores una infraestructura sólida para
la gestión de proyectos. Además de su funcionalidad de repositorio Git público, GitHub ofrece 
herramientas integrales para el seguimiento y control de eventos relacionados con el desarrollo, 
lo que facilita la colaboración eficiente y transparente entre los miembros del equipo. 
Esta plataforma ha fomentado el desarrollo comunitario, impulsando la creación y mejora de proyectos 
de software en un entorno abierto y accesible para la comunidad global de 
desarrolladores.\footnote{El control de versiones es un sistema que registra y controla los cambios 
realizados en un proyecto a lo largo del tiempo. Permite realizar un seguimiento de las modificaciones, 
gestionar conflictos y recuperar versiones anteriores del código. Git es un sistema de control de 
versiones ampliamente utilizado en el desarrollo de software.}

GitHub, como plataforma de control de versiones basada en Git, permite a los desarrolladores 
almacenar y compartir sus repositorios de código, facilitando la colaboración y la contribución 
de múltiples personas a un proyecto. Además, ofrece herramientas como problemas, solicitudes de 
extracción y seguimiento de errores que permiten una comunicación efectiva entre los miembros del 
equipo y facilitan la gestión y resolución de problemas en el proceso de desarrollo de 
software.\footnote{Los problemas y las solicitudes de extracción son mecanismos utilizados en GitHub 
para informar y abordar problemas, sugerir cambios y revisar y fusionar contribuciones de código.}

El uso de GitHub ha fomentado el desarrollo comunitario y la creación de proyectos de software 
de calidad en un entorno colaborativo y transparente. Los desarrolladores pueden contribuir 
a proyectos existentes, realizar mejoras y correcciones de errores, y beneficiarse de la 
retroalimentación y la experiencia de otros miembros de la comunidad global de desarrolladores. 
Además, GitHub facilita la visibilidad y la accesibilidad de los proyectos, lo que permite a otros 
descubrir, aprender y utilizar el software desarrollado por la comunidad.\footnote{La comunidad 
global de desarrolladores se refiere a la amplia red de personas que colaboran, comparten conocimientos 
y contribuyen al desarrollo de software en todo el mundo.}

\subsection{Integración continua y el control de calidad}

La \textit{integración continua} y el \textit{control de calidad} desempeñan un papel crucial en el desarrollo de software. 
Para garantizar la calidad y la consistencia del proyecto, se ha utilizado \textit{SonarCloud}\footnote{SonarCloud es una herramienta de control de calidad que proporciona 
análisis estático de código para identificar problemas y mejorar la calidad del código.} como herramienta 
de control de calidad. Esta herramienta se integra con GitHub, lo que permite realizar un análisis 
automatizado de la calidad del código en cada commit. SonarCloud evalúa el código fuente en función 
de los estándares de calidad predefinidos y proporciona información detallada sobre posibles 
problemas, vulnerabilidades o malas prácticas. Esta integración continua de control de calidad 
asegura que el proyecto cumpla con los criterios de calidad deseados y permite abordar los 
problemas de manera oportuna.\footnote{El control de calidad se refiere al conjunto de procesos y técnicas utilizados para asegurar 
la calidad del software.}

Además, se ha empleado \textit{GitHub Pages} como una plataforma para alojar la documentación del código 
fuente de la biblioteca generada. GitHub Pages permite crear un sitio web estático que sirve 
como una fuente centralizada de información para los usuarios y desarrolladores del proyecto. 
Al alojar la documentación en GitHub Pages, se facilita el acceso y la navegación a través de 
la documentación, lo que mejora la usabilidad y la visibilidad del proyecto. Esta práctica de 
utilizar GitHub Pages para la documentación garantiza que la información esté siempre actualizada 
y disponible para todos los interesados en el proyecto.

\subsection{Persistencia de datos}

Se ha decidido seguir la metodología establecida en el Trabajo de Fin de Grado anterior, donde se 
emplean archivos CSV para almacenar los conjuntos de datos generados. Estos archivos CSV ofrecen una 
estructura tabular que permite representar de manera eficiente la lista de enlaces de paquetes y sus 
dependencias.\footnote{CSV (\textit{Comma-Separated Values}) es un formato de archivo que utiliza comas para 
separar los valores en una estructura tabular. Es ampliamente utilizado para el intercambio de datos 
en aplicaciones que requieren una estructura tabular sencilla.}

Además de los archivos CSV, en algunos casos se ha optado por utilizar objetos serializados para el 
almacenamiento de datos. Esta elección se basa en la facilidad que brindan los objetos serializados 
para ser guardados y cargados en los entornos de desarrollo, como los Jupyter Notebooks utilizados 
en el proyecto. Al serializar los objetos, se logra una representación compacta que puede ser 
almacenada en archivos y posteriormente restaurada sin perder la integridad de los 
datos.\footnote{La serialización es el proceso de convertir un objeto en una secuencia de 
bytes que puede ser almacenada o transmitida, y posteriormente restaurada para obtener el 
objeto original. Esto facilita la persistencia de datos complejos en entornos de programación.}

Sin embargo, la masividad de los datos ha planteado desafíos en cuanto a su almacenamiento. 
El volumen de los datos generados ha requerido el empleo de técnicas de compresión y división 
en \textit{lotes} para asegurar su conservación eficiente. Mediante la compresión\footnote{La compresión 
de datos es el proceso de reducir el tamaño de un 
archivo o conjunto de datos sin perder su contenido o información. Existen diferentes 
algoritmos de compresión que se utilizan para lograr este objetivo.}, se reduce el tamaño 
de los archivos de datos sin perder su contenido, lo que permite ahorrar espacio de 
almacenamiento. Por otro lado, la división 
en lotes consiste en dividir los datos en conjuntos más pequeños, lo cual facilita su manejo y 
procesamiento en entornos con recursos limitados.

\subsection{Gestión y organización del proyecto}

Inicialmente, se establecieron reuniones presenciales quincenales para discutir los objetivos de 
los \textit{sprints} propuestos. A medida que nos acercábamos a la etapa final del proyecto, se optó por 
realizar reuniones semanales para una mayor agilidad en la toma de decisiones. Sin embargo, debido 
a la naturaleza del proyecto, gestionar adecuadamente los sprints ha sido un desafío, ya que en 
ocasiones fue necesario replantear la forma en que estábamos abordando las tareas e incluso 
retroceder para solucionar problemas que surgieron durante el proceso.\footnote{Un sprint es un 
período de tiempo durante el cual se realiza un conjunto de tareas o actividades dentro de un 
proyecto ágil. Se utiliza comúnmente en la metodología Scrum para la gestión de proyectos.}

Se ha utilizado \textit{Microsoft Teams} como herramienta para facilitar las reuniones de forma remota, 
lo que permitió una comunicación efectiva y una colaboración fluida entre los miembros del equipo. 
Esta plataforma proporcionó un espacio para compartir documentos, discutir ideas y mantener un 
seguimiento de las tareas asignadas.\footnote{Microsoft Teams es una plataforma de colaboración 
en línea que permite la comunicación y el trabajo en equipo a través de chat, videoconferencias, 
compartición de archivos y otras funcionalidades.}

A lo largo del proyecto, se pueden distinguir varias etapas. En primer lugar, hubo una fase de 
toma de contacto, donde se adquirió un conocimiento inicial sobre los objetivos y el alcance del 
Trabajo de Fin de Grado. A continuación, se llevó a cabo una fase de investigación y aprendizaje, 
donde se profundizó en los conceptos teóricos de la ciencia de redes, aprovechando los conocimientos 
adquiridos en asignaturas como \textit{Nuevas Tecnologías}.\footnote{Asignatura del grado de Ingeniería 
Informática en la UBU que proporciona una visión general de la ciencia de redes y sus aplicaciones.}

Otra etapa clave fue el estudio de estrategias para la obtención de datos. Dado que había 
diferentes fuentes disponibles, como archivos CSV, sitios web y APIs\footnote{API (Application Programming Interface) es un 
conjunto de reglas y protocolos que permite la comunicación y la interacción entre diferentes 
software o componentes de software. Permite el intercambio de datos y la ejecución de funciones 
entre sistemas diferentes.}, se exploraron y 
seleccionaron las mejores opciones para obtener los datos necesarios. Además, se desarrolló 
una herramienta en Python que permitió la extracción de datos de estas diversas fuentes de 
manera eficiente y automatizada.

Una vez obtenidos los datos, se procedió a realizar un análisis de los mismos, aplicando técnicas 
y algoritmos propios de la ciencia de redes para extraer información relevante y obtener conclusiones 
significativas. Este análisis proporcionó una base sólida para la posterior redacción de la memoria 
del proyecto.