\capitulo{4}{Técnicas y herramientas}


\section{Análisis de los datos disponibles}

Para llevar a cabo el análisis de las redes de dependencias de paquetes de software, contamos con 
datos proporcionados por \textit{libraries.io}\cite{jeremy_katz_2020_3626071} en formato CSV. Estos 
datos nos permiten extraer la red dedependencias de los principales repositorios de paquetes que 
vamos a estudiar en este trabajo
\textit{(CRAN, PyPI, npm)}\footnote{Algunos repositorios incluidos en los conjuntos de datos 
son: npm, Maven, PyPI, CRAN, Rubygems, Packagist, NuGet}. Sin embargo,
es importante destacar que estos datos no están actualizados a la fecha actual debido a la
elevada evolucion de los cambios en los proyectos de software. Por lo tanto, los análisis que
realicemos con estos datos no reflejarán la situación actual y actualizada de la red de dependencias.

A pesar de esta limitación, contamos con un conjunto de datos que abarca un gran número de repositorios
y un histórico de las dependencias de los paquetes para sus distintas versiones. Esto nos permite
realizar análisis retrospectivos y estudiar la evolución de las dependencias a lo largo del
tiempo.

En conclusión, es evidente la necesidad de obtener un nuevo conjunto de datos que nos brinde
un estudio más preciso y actualizado de las relaciones de paquetes en los repositorios. Dado
que tanto el conjunto de datos de \textit{libraries.io} como su API tiene ciertas limitaciones, se ha considerado como una opción
secundaria dando preferencia a otras técnicas de extracción de datos como el \textit{web scraping}.

\subsection{Web Scraping}

Para obtener un conjunto de datos actualizado, se plantea la posibilidad de realizar un
\textit{web scraping} de los repositorios de paquetes de software. El \textit{web scraping}
es una técnica que consiste en extraer información de sitios web de manera automatizada.
Esta técnica nos permite obtener datos de los repositorios de paquetes de software y
construir un conjunto de datos actualizado.

Sin embargo, el \textit{web scraping} presenta ciertas limitaciones que dificultan su uso
para la extracción de datos. A continuación, se presentan las principales limitaciones del
\textit{web scraping}:

\begin{itemize}
    \item \textbf{Estructura de los sitios web}: Los sitios web no están diseñados para
          ser analizados por máquinas, sino para ser visualizados por humanos. Por lo tanto,
          la estructura de los sitios web puede cambiar con frecuencia, lo que dificulta
          el proceso de extracción de datos.
    \item \textbf{Detección de bots}: Algunos sitios web pueden detectar el uso de bots
          y bloquear las solicitudes. Esto puede ocurrir si se realizan demasiadas solicitudes
          en un período de tiempo corto.
    \item \textbf{Limitaciones de velocidad}: Algunos sitios web pueden limitar la velocidad
          de las solicitudes para evitar una carga excesiva en los servidores. Esto puede
          provocar que el proceso de extracción de datos sea más lento y menos eficiente.
\end{itemize}

Es importante tener en cuenta estas limitaciones al utilizar el \textit{web scraping} para
asegurar un uso adecuado y obtener los datos necesarios.

El web scraping es una técnica que puede aparecer combinada con otras técnicas de extracción
de datos, como la API de \textit{libraries.io} o las APIS propias de los gestores de paquetes. 

\subsection{Exploración de los repositorios de paquetes}

La exploración de los repositorios de paquetes desempeña un papel fundamental en nuestro estudio.
A continuación, se presentan los puntos basicos a tener en cuenta en esta temática y en los que se
centra el análisis:

\begin{itemize}
    \item \textbf{Diversidad de repositorios}: Existe una amplia gama de repositorios de paquetes,
          cada uno especializado en un lenguaje de programación o plataforma específica. Es esencial
          conocer la variedad de repositorios relevantes para nuestra área de interés, como npm para JavaScript,
          PyPI para Python o Maven para Java. Cada repositorio tiene su propia comunidad, conjunto de reglas
          y mejores prácticas.
    \item \textbf{Estructura y metadatos de los paquetes}: Cada paquete de software en un repositorio
          está acompañado de metadatos que proporcionan información importante. Estos metadatos incluyen el
          nombre del paquete, la versión, la descripción, el autor, la licencia, las dependencias y más.
          Estos son los datos relevantes para nuestra investigación y en los que nos centraremos durante
          la extracción\footnote{Los metadatos varían entre los diferentes repositorios y pueden contener
              información adicional específica del repositorio en cuestión.}.
    \item \textbf{Versionado y mantenimiento}: Los repositorios de paquetes suelen gestionar diferentes
          versiones de un paquete, cada una con sus propias mejoras, correcciones de errores y posibles cambios
          en las dependencias. El seguimiento y análisis de los patrones de versionado y las prácticas de
          mantenimiento son aspectos esenciales para comprender la evolución de los paquetes a lo largo del
          tiempo\footnote{El versionado de paquetes sigue diferentes convenciones dependiendo del repositorio
              y las prácticas adoptadas por los desarrolladores del software.}.
\end{itemize}

\subsection{API de libraries.io}

\textit{Libraries.io} proporciona una API para acceder a los datos de los repositorios de paquetes
de software. Esta API nos permite obtener información sobre los paquetes y sus dependencias,
así como también información sobre los repositorios y sus características.

La API de \textit{libraries.io} es una herramienta muy útil para obtener información sobre los
paquetes y repositorios de software. Sin embargo, presenta algunas limitaciones que dificultan
su uso para la extracción de datos. A continuación, se presentan las principales limitaciones
de la API:

\begin{itemize}
    \item \textbf{Autenticación}: Para realizar cualquier solicitud a la API, es necesario
          incluir el parámetro \textit{api\_key} que se obtiene desde la página de tu cuenta.
          Esto implica un registro en el servicio.
    \item \textbf{Distintas versiones del servicio}: Existen diferentes versiones del servicio,
          siendo la versión gratuita la que ofrece características básicas, mientras que las características
          adicionales están disponibles a través de planes de pago.
    \item \textbf{Límite de velocidad}: Todas las solicitudes están sujetas a un límite de
          velocidad (de 60 solicitudes por minuto en la versión gratuita) basado en tu \textit{api\_key}.
          Si se realizan más solicitudes dentro de ese período de tiempo, se recibirá una respuesta de error 429. 
          Este límite está diseñado para garantizar un uso equitativo de los recursos y evitar una carga 
          excesiva en los servidores.
    \item \textbf{Paginación}: Algunas solicitudes pueden devolver múltiples resultados, y para
          manejar estos casos, se puede utilizar la paginación. Se pueden utilizar los parámetros de
          consulta \textit{page} y \textit{per\_page} para controlar la cantidad de resultados
          por página. El valor predeterminado de \textit{page} es 1 y el valor predeterminado
          de \textit{per\_page} es 30 resultados por página. Esto podría implicar resultados truncados,
          lo cual no es deseable.
\end{itemize}

\subsection{NetworkX}

La librería \emph{NetworkX}, implementada en el lenguaje de programación \emph{Python}, se presenta 
como una poderosa herramienta para el análisis de redes. Su notable versatilidad y amplio conjunto de
 funcionalidades la han consolidado como una elección destacada en diversos ámbitos, que abarcan desde
  la ciencia de datos y la investigación científica hasta la ingeniería y la sociología.

La librería \emph{NetworkX} proporciona una interfaz flexible e intuitiva que permite la creación, 
manipulación y visualización de redes. Estas redes, que pueden ser representadas como grafos dirigidos 
o no dirigidos, contemplan nodos como entidades y enlaces o aristas que denotan las relaciones 
existentes entre ellas.

Además de las operaciones fundamentales para la construcción y manipulación de grafos, \emph{NetworkX} se
destaca por ofrecer una amplia gama de algoritmos y métricas específicamente diseñados para el análisis de redes. 
Estos algoritmos permiten realizar tareas tales como la detección de comunidades, la evaluación de la centralidad de 
los nodos, la búsqueda de caminos más cortos y la identificación de estructuras relevantes dentro de la red.

La capacidad de visualización constituye otro aspecto sobresaliente de \emph{NetworkX}, ya que proporciona 
diversas alternativas para representar gráficamente las redes. Estas opciones visuales favorecen la comprensión 
y comunicación de los resultados obtenidos a partir del análisis de redes, permitiendo además la personalización 
de atributos visuales, como el color y tamaño de los nodos, con el fin de resaltar características particulares 
de la red en cuestión.

\section{Entorno de desarrollo}

Se ha trabajado en un sistema operativo \textit{Ubuntu} y utilizando \textit{Visual Studio Code} 
(VSCode) como entorno de desarrollo integrado (IDE) preferido\footnote{ La elección del sistema 
operativo y el IDE depende de las preferencias personales del autor}. VSCode se destaca por 
su versatilidad y extensibilidad, lo que me permite personalizarlo y adaptarlo a mis necesidades 
específicas. Su amplia selección de extensiones proporcionan herramientas adicionales y funcionalidades 
especializadas que enriquecen la experiencia de programación.

El lenguaje de programación elegido es \textit{Python}, reconocido por su facilidad de uso y amplia 
gama de bibliotecas y frameworks disponibles. Sin embargo, también considero esencial el uso de \textit{Bash}, 
un intérprete de comandos de Unix, para realizar diversas tareas y automatizaciones en el entorno 
de desarrollo.

Para realizar el proyecto, cuento con un ordenador portátil de gama media equipado con un p
rocesador \textit{Intel® Core™ i5-11400H} y 16 GB de RAM. Estas especificaciones brindan un rendimiento 
adecuado para el desarrollo y la ejecución del software que acostumbro a usar\footnote{ Aunque el 
rendimiento puede variar dependiendo de los requisitos específicos del proyecto}. Sin embargo, en 
algunos casos ha habido incidentes debido al elevado consumo de memoria que requiere el procesamiento 
de la masiva cantidad de datos a la que nos hemos enfrentado.

A continuación, se presentan algunas bibliotecas y herramientas utilizadas en el desarrollo de proyectos de software:

\subsection{Pandas}
Pandas es una biblioteca de análisis de datos de alto rendimiento que proporciona estructuras de datos y herramientas para manipular y analizar conjuntos de datos complejos.

\subsection{tqdm}
tqdm es una biblioteca que agrega una barra de progreso elegante y visual a los bucles iterativos, lo que facilita el seguimiento del progreso de las operaciones en tiempo real.

\subsection{Requests}
Requests es una biblioteca que simplifica el manejo de solicitudes HTTP, permitiendo realizar peticiones a servidores web y recibir respuestas de manera sencilla.

\subsection{BeautifulSoup4}
BeautifulSoup4 es una biblioteca utilizada para extraer información de páginas web y realizar el análisis de datos web. Facilita la extracción de datos estructurados y no estructurados mediante técnicas de web scraping.

\subsection{Selenium}
Selenium es una biblioteca que automatiza la interacción con navegadores web, lo que permite realizar pruebas de aplicaciones web o realizar acciones específicas en páginas web de forma programática.

\subsection{Networkx}
Networkx es una biblioteca para el análisis de redes y grafos. Proporciona herramientas para la creación, manipulación y estudio de estructuras de redes complejas.

\subsection{Matplotlib}
Matplotlib es una biblioteca de visualización de datos en 2D que permite crear gráficos y visualizaciones de datos de alta calidad.

\subsection{Pybraries}
Pybraries es una biblioteca que actúa como un wrapper del API de libraries.io para Python.

\subsection{Typing\_extensions}
Typing\_extensions es una extensión del módulo typing de Python que proporciona funcionalidades adicionales para anotaciones de tipos en tiempo de ejecución.

\subsection{pdoc}
pdoc es una biblioteca que permite generar documentación automática a partir de los archivos de código fuente.

\section{Computación en la nube: \textit{Kaggle} y \textit{Deepnote}}

\textit{Jupyter Notebooks} se ha convertido en una herramienta fundamental en el ámbito de la ciencia de datos y 
la programación interactiva. Estos notebooks permiten combinar código, texto explicativo y resultados 
visuales en un solo documento, lo que facilita la comunicación y colaboración en proyectos de análisis 
de datos. Los notebooks se ejecutan en un entorno interactivo, lo que permite explorar y experimentar 
con el código de manera iterativa, lo que resulta especialmente útil en tareas de análisis exploratorio 
de datos.

En cuanto a la computación en la nube, ha desempeñado un papel clave en el desarrollo de este TFG.
Plataformas como \textit{Kaggle} o \textit{Deepnote} proporcionan servicios de notebooks basados en la nube, lo que 
significa que los usuarios pueden acceder a un entorno de desarrollo completo sin tener que preocuparse 
por configurar y mantener su propia infraestructura. Esto es especialmente beneficioso en proyectos 
que requieren una gran cantidad de recursos computacionales, como el procesamiento de grandes volúmenes
 de datos.

Además, la computación en la nube ha ayudado a reducir los costos asociados con la obtención y 
procesamiento de datos. La obtención de datos puede requerir tiempo, memoria y almacenamiento 
significativos, lo que puede ser costoso en términos de recursos locales. Al aprovechar la computación 
en la nube podemos acceder a recursos escalables y flexibles según 
sea necesario, lo que nos permite realizar análisis más eficientes y a gran escala sin incurrir en 
costos excesivos.\footnote{La escalabilidad y flexibilidad de los recursos en la computación en la 
nube se refiere a la capacidad de aumentar o disminuir la capacidad de cómputo y almacenamiento 
según las necesidades del proyecto, lo que permite un uso más eficiente de los recursos y un mejor 
control de costos.}

\section{Sistema de control de versiones}

\textit{GitHub} ha desempeñado un papel fundamental como plataforma de control de versiones Git en el ámbito del 
desarrollo de software colaborativo de código abierto. Como un estándar reconocido internacionalmente, 
GitHub, adquirido por \textit{Microsoft}, ha proporcionado a los desarrolladores una infraestructura sólida para
la gestión de proyectos. Además de su funcionalidad de repositorio Git público, GitHub ofrece 
herramientas integrales para el seguimiento y control de eventos relacionados con el desarrollo, 
lo que facilita la colaboración eficiente y transparente entre los miembros del equipo. 
Esta plataforma ha fomentado el desarrollo comunitario, impulsando la creación y mejora de proyectos 
de software en un entorno abierto y accesible para la comunidad global de 
desarrolladores.

GitHub, como plataforma de control de versiones basada en Git, permite a los desarrolladores 
almacenar y compartir sus repositorios de código, facilitando la colaboración y la contribución 
de múltiples personas a un proyecto. Además, ofrece herramientas como problemas, solicitudes de 
extracción y seguimiento de errores que permiten una comunicación efectiva entre los miembros del 
equipo y facilitan la gestión y resolución de problemas en el proceso de desarrollo de 
software.

El uso de GitHub ha fomentado el desarrollo comunitario y la creación de proyectos de software 
de calidad en un entorno colaborativo y transparente. Los desarrolladores pueden contribuir 
a proyectos existentes, realizar mejoras y correcciones de errores, y beneficiarse de la 
retroalimentación y la experiencia de otros miembros de la comunidad global de desarrolladores. 
Además, GitHub facilita la visibilidad y la accesibilidad de los proyectos, lo que permite a otros 
descubrir, aprender y utilizar el software desarrollado por la comunidad.

\section{Integración continua y el control de calidad}

La \textit{integración continua} y el \textit{control de calidad} desempeñan un papel crucial en el desarrollo de software. 
Para garantizar la calidad y la consistencia del proyecto, se ha utilizado \textit{SonarCloud}\footnote{SonarCloud es una herramienta de control de calidad que proporciona 
análisis estático de código para identificar problemas y mejorar la calidad del código.} como herramienta 
de control de calidad. Esta herramienta se integra con GitHub, lo que permite realizar un análisis 
automatizado de la calidad del código en cada commit. SonarCloud evalúa el código fuente en función 
de los estándares de calidad predefinidos y proporciona información detallada sobre posibles 
problemas, vulnerabilidades o malas prácticas. Esta integración continua de control de calidad 
asegura que el proyecto cumpla con los criterios de calidad deseados y permite abordar los 
problemas de manera oportuna.\footnote{El control de calidad se refiere al conjunto de procesos y técnicas utilizados para asegurar 
la calidad del software.}

Además, se ha empleado \textit{GitHub Pages} como una plataforma para alojar la documentación del código 
fuente de la biblioteca generada. GitHub Pages permite crear un sitio web estático que sirve 
como una fuente centralizada de información para los usuarios y desarrolladores del proyecto. 
Al alojar la documentación en GitHub Pages, se facilita el acceso y la navegación a través de 
la documentación, lo que mejora la usabilidad y la visibilidad del proyecto. Esta práctica de 
utilizar GitHub Pages para la documentación garantiza que la información esté siempre actualizada 
y disponible para todos los interesados en el proyecto.

\section{Persistencia de datos}

Se ha decidido seguir la metodología establecida en el Trabajo de Fin de Grado anterior, donde se 
emplean archivos CSV para almacenar los conjuntos de datos generados. Estos archivos CSV ofrecen una 
estructura tabular que permite representar de manera eficiente la lista de enlaces de paquetes y sus 
dependencias.\footnote{CSV (\textit{Comma-Separated Values}) es un formato de archivo que utiliza comas para 
separar los valores en una estructura tabular. Es ampliamente utilizado para el intercambio de datos 
en aplicaciones que requieren una estructura tabular sencilla.}

Además de los archivos CSV, en algunos casos se ha optado por utilizar objetos serializados para el 
almacenamiento de datos. Esta elección se basa en la facilidad que proporcionan los objetos serializados 
para ser guardados y cargados en los entornos de desarrollo, como los Jupyter Notebooks utilizados 
en el proyecto. Al serializar los objetos, se logra una representación compacta que puede ser 
almacenada en archivos y posteriormente restaurada sin perder la integridad de los 
datos.\footnote{La serialización es el proceso de convertir un objeto en una secuencia de 
bytes que puede ser almacenada o transmitida, y posteriormente restaurada para obtener el 
objeto original. Esto facilita la persistencia de datos complejos en entornos de programación.}

Sin embargo, el gran volumen de los datos ha planteado desafíos en cuanto a su almacenamiento. 
El volumen de los datos generados ha requerido el empleo de técnicas de compresión y división 
en \textit{lotes} para asegurar su conservación eficiente. Mediante la compresión\footnote{La compresión 
de datos es el proceso de reducir el tamaño de un 
archivo o conjunto de datos sin perder su contenido o información. Existen diferentes 
algoritmos de compresión que se utilizan para lograr este objetivo.}, se reduce el tamaño 
de los archivos de datos sin perder su contenido, lo que permite ahorrar espacio de 
almacenamiento. Por otro lado, la división 
en lotes consiste en dividir los datos en conjuntos más pequeños, lo cual facilita su manejo y 
procesamiento en entornos con recursos limitados.

\section{Gestión y organización del proyecto}

Inicialmente, se establecieron reuniones presenciales quincenales para discutir los objetivos de 
los \textit{sprints} propuestos. A medida que nos acercábamos a la etapa final del proyecto, se optó por 
realizar reuniones semanales para una mayor agilidad en la toma de decisiones. Sin embargo, debido 
a la naturaleza del proyecto, gestionar adecuadamente los sprints ha sido un desafío, ya que en 
ocasiones fue necesario replantear la forma en que estábamos abordando las tareas e incluso 
retroceder para solucionar problemas que surgieron durante el proceso.\footnote{Un sprint es un 
período de tiempo durante el cual se realiza un conjunto de tareas o actividades dentro de un 
proyecto ágil. Se utiliza comúnmente en la metodología Scrum para la gestión de proyectos.}

Se ha utilizado \textit{Microsoft Teams} como herramienta para facilitar las reuniones de forma remota, 
lo que permitió una comunicación efectiva y una colaboración fluida entre los miembros del equipo. 
Esta plataforma proporcionó un espacio para compartir documentos, discutir ideas y mantener un 
seguimiento de las tareas asignadas.

A lo largo del proyecto, se pueden distinguir varias etapas. En primer lugar, hubo una fase de 
toma de contacto, donde se adquirió un conocimiento inicial sobre los objetivos y el alcance del 
Trabajo de Fin de Grado. A continuación, se llevó a cabo una fase de investigación y aprendizaje, 
donde se profundizó en los conceptos teóricos de la ciencia de redes, aprovechando los conocimientos 
adquiridos en asignaturas como \textit{Nuevas Tecnologías}.\footnote{Asignatura del grado de Ingeniería 
Informática en la UBU que proporciona una visión general de la ciencia de redes y sus aplicaciones.}

Otra etapa clave fue el estudio de estrategias para la obtención de datos. Dado que había 
diferentes fuentes disponibles, como archivos CSV, sitios web y APIs, se exploraron y 
seleccionaron las mejores opciones para obtener los datos necesarios. Además, se desarrolló 
una herramienta en Python que permitió la extracción de datos de estas diversas fuentes de 
manera eficiente y automatizada.

Una vez obtenidos los datos, se procedió a realizar un análisis de los mismos, aplicando técnicas 
y algoritmos propios de la ciencia de redes para extraer información relevante y obtener conclusiones 
significativas. Este análisis proporcionó una base sólida para la posterior redacción de la memoria 
del proyecto.