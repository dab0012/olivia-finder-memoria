\apendice{Plan de Proyecto Software}

\section{Introducción}

El proyecto propuesto se centra en el desarrollo de la herramienta \textit{OLIVIA-Finder}, 
la cual ha sido diseñada con el objetivo de extraer datos de paquetes y sus dependencias de 
los repositorios de software \textit{CRAN, Bioconductor, PyPI y npm}. 

Dado que la herramienta en sí misma carece de elementos visuales o atractivos más allá 
de la presentación de los datos en forma de una lista de enlaces entre nodos de la red, 
se ha decidido complementarla con un análisis básico de las redes generadas desde la perspectiva 
de la ciencia de redes. Esto se realiza con el propósito de evitar que el trabajo resulte monótono 
y brindar un enfoque más completo y enriquecedor al proyecto.

En general, el proyecto se puede describir en tres etapas fundamentales. 
La primera etapa consiste en una exhaustiva investigación y documentación, donde se realiza un estudio 
en profundidad de los repositorios \textit{CRAN, Bioconductor, PyPI y npm}, así como de las técnicas y herramientas 
utilizadas para la extracción de datos y análisis de redes de dependencias. Esta etapa sienta las bases teóricas 
necesarias para comprender el contexto en el que se desarrollará la herramienta y el análisis posterior.

La segunda etapa se enfoca en el diseño y desarrollo de la herramienta \textit{OLIVIA-Finder}. 
Aquí, se aplican los conocimientos adquiridos durante la etapa de investigación para implementar una solución 
eficiente y robusta que permita la extracción de datos de los repositorios mencionados. 
Se deben tener en cuenta diversos aspectos técnicos, como el manejo de solicitudes web, el procesamiento de datos 
y la manipulación de estructuras de red.

La tercera etapa del proyecto implica el análisis experimental de las redes generadas. 
Una vez obtenidos los datos de los paquetes y sus dependencias, se aplican técnicas de la ciencia de redes 
para examinar características importantes, como la \textit{centralidad de grado}, el algoritmo \textit{PageRank} y 
otras métricas relevantes. Este análisis proporciona una comprensión más profunda de la estructura 
y las propiedades de las redes de dependencias en los repositorios estudiados, permitiendo identificar 
paquetes críticos, vulnerabilidades potenciales y relaciones significativas entre los elementos de la red.

\section{Planificación temporal}

En el ámbito de los proyectos modernos de software, es común utilizar metodologías ágiles, como \textit{Scrum} 
o \textit{Kanban}. Estas metodologías reconocen la naturaleza dinámica del proceso iterativo que implica el 
establecimiento de requisitos, diseño, desarrollo y validación de un producto de software. Por lo tanto, 
se enfocan en la planificación adaptativa y la mejora continua a través de entregas tempranas.

Sin embargo, en nuestro caso, debemos cuestionar la aplicabilidad práctica de las metodologías ágiles 
tal como están concebidas. Esto se debe a que el conjunto de partes interesadas, que se limita a un 
cliente académico prototípico, y sobre todo al hecho de que el equipo de trabajo es unipersonal. Estas 
circunstancias particulares plantean dudas sobre la eficacia de la implementación de las metodologías 
ágiles en nuestro contexto.

No obstante, podemos establecer similitudes entre nuestro proceso y el marco de trabajo \textit{Scrum}, 
debido a la presencia de \textit{sprints}. Aunque no hemos seguido rigurosamente la estructura de los \textit{sprints} 
debido a la falta de objetivos claramente definidos en el proyecto, los \textit{sprints} nos han permitido 
representar la actividad realizada y las etapas en las que hemos dividido el trabajo.

Por otro lado, se ha de tener en cuenta que las tareas de investigación incluidas en el proyecto son 
difíciles de planificar. El proceso de investigación implica continuos replanteamientos y el alcance de 
los resultados debe ser constantemente modelado o redefinido a medida que avanzamos dentro del límite 
temporal del que se dispone.

\subsection{Sprint 0: Reunión inicial}

Este \textit{sprint} representó nuestra primera aproximación a la temática del proyecto. Desde el principio, quedó 
claro que el modelo matemático proporcionado por OLIVIA en el Trabajo de Fin de Grado anterior\footnote{\url{https://github.com/dsr0018/olivia}}  estaba más allá de 
nuestra comprensión absoluta\cite{Seto-Rey20231}, y nuestro enfoque se centraría principalmente en la extracción de datos 
de dependencias de paquetes software de distintos repositorios \textit{(PyPI, Bioconductor, CRAN, npm)} y el análisis comparativo de 
la evolución temporal del nuevo conjunto de datos de dependencias entre paquetes generados.

Por lo tanto, fue necesario realizar un esfuerzo inicial para comprender los fundamentos básicos de lo que el modelo 
de \textit{OLIVIA} nos permitía hacer. En este sentido, los Jupyter Notebooks proporcionados en el TFG anterior fueron de 
gran utilidad, ya que mostraban la funcionalidad en casos de estudio concretos y ofrecían un análisis complementario.

Desde el principio, nos encontramos con problemas técnicos. En primer lugar, las dependencias de \textit{OLIVIA} requieren una 
actualización, ya que se están utilizando versiones algo desactualizadas de algunos paquetes y herramientas de análisis, 
y el \textit{Dependabot de GitHub} insiste en su actualización \textit{``Bump numpy from 1.18.5 to 1.22.0''}. En concreto, 
se ha identificado que esta versión de \textit{NumPy} presenta vulnerabilidades de alto riesgo, como la referida al 
\textit{NumPy NULL Pointer Dereference}.

Sin embargo, no es posible actualizar la biblioteca, ya que existen métodos esenciales para la funcionalidad 
implementada en \textit{OLIVIA} que se han vuelto obsoletos en la versión actualizada. Además, en cuanto a la versión de Python, 
el código debe ejecutarse en la versión 3.8 debido a la presencia de dependencias clave para OLIVIA, como \textit{intbitset}, 
que no están disponibles para otras versiones de Python. Esto supuso un problema al ejecutar los notebooks 
en \textit{Google Colab} debido a las dificultades para instalar la versión específica de Python que requeríamos.

Además, fue necesario realizar una introducción a la temática mediante una lectura superficial de la memoria del 
TFG de OLIVIA. Esto también nos permitió familiarizarnos con el hecho de que estas memorias se redactan 
utilizando \LaTeX, una tecnología que nos resultaba completamente desconocida en ese momento.

En conclusión, este \textit{sprint} inicial implicó la familiarización con el modelo de OLIVIA, la comprensión de 
sus fundamentos y la resolución de desafíos técnicos relacionados con las dependencias y las versiones de Python. 
Además, se llevó a cabo una introducción a la temática a través del estudio de la memoria del TFG anterior, lo que 
también nos permitió adquirir conocimientos sobre el uso de \LaTeX en la redacción de documentos científicos.

La duración de este \textit{sprint} ha sido de 30 días aproximadamente, realizando 2 reuniones y 
con unas 25 horas de trabajo.

\subsection{Sprint 1 - Recolección de datos}

En esta etapa de investigación nos enfocamos en el manejo de datos. Inicialmente, utilizamos el conjunto de datos 
de \textit{Libraries.io}\cite{jeremy_katz_2020_3626071}, el cual resultó útil desde una perspectiva histórica. Sin embargo, presentó importantes 
limitaciones, como la falta de actualización periódica. Al enfrentarnos a los desafíos derivados de este conjunto 
de datos, pudimos constatar que trabajar con grandes volúmenes de datos no es trivial. Fue necesario utilizar un 
disco duro externo para almacenar el conjunto de datos, del cual sólo nos interesaba la lista de enlaces entre 
paquetes y dependencias contenida en uno de los archivos CSV. Debido a la gran cantidad de líneas y su considerable 
tamaño, resultó difícil manipular y filtrar los datos.

La falta de actualización de los datos fue un aspecto clave a mejorar, por lo cual exploramos otras fuentes de información, 
como la API de \textit{Libraries.io} y el conjunto de datos de \textit{BigQuery} proporcionado por los mismos desarrolladores. 
El uso de la API quedó descartado debido a sus limitaciones técnicas para realizar un escaneo completo del repositorio. 
Por otro lado, el conjunto de datos de \textit{BigQuery} también presentó problemas similares a los archivos CSV en 
términos de falta de actualización.

Como alternativa, se propuso utilizar técnicas de \textit{web scraping} para recolectar información de los sitios web 
de los repositorios de software de nuestro interés, comenzando con la recolección de información de CRAN. Este esfuerzo 
dio resultados positivos, y logramos obtener una lista actualizada de la red de CRAN. Los éxitos obtenidos en este 
proceso nos llevaron a considerar el desarrollo de una herramienta que pudiera obtener esta información para los 
repositorios de interés, ya que esto generaría un nuevo conjunto de datos utilizable en OLIVIA y en trabajos anteriores 
que hayan utilizado datos de \textit{Libraries.io}, lo que permitiría actualizar sus resultados de manera significativa.

Como resultado al finalizar este \textit{sprint}, logramos obtener una red actualizada de CRAN gracias a que desarrollamos 
un prototipo básico pero todavía inmaduro de un código en Python que nos permitió realizar la recolección de datos.

Es importante destacar que durante este \textit{sprint} se abordó uno de los principales desafíos que hemos enfrentado. 
Como era de esperar, los servidores web implementan medidas de seguridad para evitar comportamientos maliciosos. Debido 
a la naturaleza del proceso de escaneo de un repositorio, que implica realizar numerosas solicitudes web a un mismo servidor 
desde una misma dirección IP en un período de tiempo relativamente corto, esta actividad a menudo resulta en la prohibición temporal de acceso al servidor web para los equipos asociados a esa dirección IP. Para solucionar este problema, se implementó la funcionalidad de ocultar las solicitudes detrás de servidores proxy, que enmascaran la dirección IP de origen al servidor web. Por lo tanto, fue necesario desarrollar esta funcionalidad para que fuera posible llevar a cabo esta tarea.

En conclusión, este \textit{sprint} fue clave para obtener un conjunto de datos actualizado y establecer los primeros 
pasos en el desarrollo de una herramienta de recolección de datos. Además, se logró resolver el desafío de la prohibición 
de acceso a los servidores web mediante la implementación de rotación de proxy. Estos avances sientan las bases para continuar con el desarrollo 
del proyecto y alcanzar los objetivos planteados.

La duración de este \textit{sprint} ha sido aproximadamente de 30 días, que en nuestra metodología de trabajo equivalen 
aproximadamente a 2 reuniones y a 60 horas de trabajo.

\subsection{Sprint 2 - Implementación de la biblioteca de extracción de datos}

En esta etapa, nuestro enfoque se centró en el desarrollo de una herramienta genérica con el propósito de llevar a cabo 
una extracción sencilla de la red de dependencias de los repositorios \textit{CRAN}, \textit{Bioconductor}, \textit{PyPI} 
y \textit{npm}. Nos enfrentamos al primer desafío de obtener una lista completa de los paquetes disponibles en cada 
repositorio, que serviría como punto de partida para nuestra recopilación de datos. Cabe destacar que esta tarea no 
resulta sencilla en la mayoría de los casos, ya que dichas listas no siempre están disponibles.

Es importante mencionar que cada repositorio de software presenta sus peculiaridades distintivas. En el caso particular 
de \textit{CRAN} y \textit{Bioconductor}, el proceso se basó exclusivamente en técnicas de \textit{web scraping}, 
aprovechando los datos de interés que se encuentran directamente en las listas de paquetes publicadas en sus respectivos 
sitios web.

El análisis de \textit{CRAN} resultó ser el más sencillo de todos, ya que su página web es simple, robusta y aparentemente 
sólida a lo largo del tiempo, lo que proporciona una implementación bastante estable para este repositorio. Por otro 
lado, inicialmente se esperaba que \textit{Bioconductor} fuese más sencillo debido a la menor cantidad de paquetes en 
comparación con los otros repositorios en los que estamos trabajando. Sin embargo, nos encontramos con un problema en 
el servidor web de \textit{Bioconductor}, específicamente en la página que muestra el listado de paquetes disponibles, 
ya que no era accesible mediante una simple solicitud web, como las que solemos realizar utilizando la biblioteca 
\textit{requests} de Python.

En \textit{Bioconductor}, se utilizaba JavaScript para cargar dinámicamente la lista de paquetes en tiempo de ejecución 
sobre la página. Esta situación dificulta la obtención de los datos, ya que la biblioteca \textit{requests} no es 
compatible con la carga de JavaScript. Como alternativa, tuvimos que recurrir a la biblioteca \textit{selenium}, la 
cual ofrece funcionalidades más avanzadas al actuar como un navegador \textit{headless} (sin interfaz gráfica) que 
se comporta de manera similar a un navegador de escritorio convencional al que estamos acostumbrados, pero que admite 
la automatización de tareas. Gracias a \textit{selenium}, logramos extraer la lista de paquetes de \textit{Bioconductor} 
y, a partir de ella, procedimos de manera similar a como lo habíamos hecho anteriormente, obteniendo los datos de interés 
mediante técnicas de \textit{web scraping}.

El repositorio \textit{PyPI}, también publica una lista de paquetes. Sin embargo, es importante destacar que esta 
lista contiene muchos paquetes obsoletos e inexistentes, lo que la convierte en un punto de partida necesario pero 
no del todo óptimo. Utilizando esta lista de paquetes y aprovechando la API proporcionada por \textit{PyPI} para 
obtener metadatos de los paquetes, pudimos extraer la red de dependencias correspondiente.

En esta etapa, nos percatamos del tiempo considerablemente elevado requerido para llevar a cabo esta recopilación, 
así como de la necesidad de cuidar la implementación de la herramienta para evitar el desperdicio de memoria. 
Estos problemas surgidos debido al tamaño de los datos nos proporcionan una perspectiva clara de la importancia de 
gestionar eficientemente los recursos cuando se trabaja con cantidades masivas de información. En cuanto al tiempo 
necesario para la recolección, se realizó un esfuerzo por optimizar el proceso mediante técnicas de concurrencia, 
lo cual nos permitió realizar solicitudes web de forma concurrente en lugar de secuencial, como habíamos estado haciendo 
hasta ese momento. Estas mejoras significativas en el rendimiento de la herramienta se tradujeron en una reducción 
significativa del tiempo requerido y el consumo de memoria.

Finalmente, logramos generar el conjunto de datos para el repositorio \textit{npm}. Es importante destacar que este 
repositorio ha sido el más desafiante de abordar. En primer lugar, no existe una forma sencilla de obtener una lista 
completa de los paquetes existentes en \textit{npm}. Además, el repositorio oficial de paquetes no es único, ya que 
existen repositorios espejo (\textit{mirrors}) alternativos que difieren tanto en el número de paquetes como en los 
paquetes que contienen. Para abordar este problema, decidimos utilizar la lista de paquetes que pudimos extraer de 
uno de estos \textit{mirrors} y complementar con los paquetes que teníamos en el conjunto de datos de \textit{Libraries.io}. 
De esta manera, obtuvimos una lista de nombres de paquetes a los cuales dirigir nuestros esfuerzos. A partir de esta 
lista, aplicamos la metodología utilizada previamente para el resto de los repositorios. En el caso de \textit{npm}, 
gracias a su API, pudimos recopilar los metadatos necesarios para construir el conjunto de datos correspondiente.

Llevar a cabo este \textit{sprint} ha supuesto una duración de 60 días, lo que aproximadamente corresponde con unas 
3 reuniones y alrededor de 150 horas de trabajo.

\subsection{Sprint 3 - Refactorización de la biblioteca}

Tras el análisis de los datos recolectados, observamos que los conjuntos de datos de \textit{Libraries.io} proporcionan 
información sobre todas las versiones existentes de un paquete. En otras palabras, consideramos como \textit{dependencias} 
de un paquete todas las dependencias que hayan existido para cada una de sus versiones. Sin embargo, desde una perspectiva 
de desarrollo de software, esto no es correcto, ya que sobrecargamos las dependencias de un paquete con dependencias de 
versiones antiguas que ya no se utilizan en el ciclo de vida actual de esas bibliotecas. Por lo tanto, es importante tener 
en cuenta esta información en los análisis posteriores que realicemos, donde será necesario aplicar un filtrado adecuado 
si deseamos utilizar los datos de \textit{Libraries.io}.
Otro aspecto interesante es que no todas las \textit{dependencias} de un paquete se encuentran presentes en el repositorio 
al que pertenece ese paquete, e incluso es posible que no utilicen el mismo lenguaje de programación. Tomemos como ejemplo 
un paquete en Python que depende de un binario escrito en C. Este fenómeno es muy común en la red de \textit{Bioconductor}, 
cuyos paquetes dependen en gran medida de paquetes existentes en \textit{CRAN}. 
A estas dependencias las hemos bautizado como \textit{dependencias foráneas}.

Con el objetivo de mejorar la funcionalidad y flexibilidad de la biblioteca, se llevó a cabo una refactorización del 
diseño para adaptarlo y permitir el uso y la combinación de diversas fuentes de datos. Se proporciona soporte para 
\textit{web scraping}, conjuntos de datos en formato \textit{CSV}, la API de \textit{Libraries.io} y repositorios de 
\textit{GitHub}.
El uso combinado de diferentes fuentes de datos nos brinda la capacidad de buscar en fuentes alternativas cuando un 
paquete solicitado no se encuentra en la fuente principal. El soporte de archivos \textit{CSV} nos permite considerar 
los conjuntos de datos de \textit{Libraries.io} como fuente de información, tanto a través de su API como de los archivos 
en sí. La implementación para repositorios de \textit{GitHub} nos proporciona información adicional al utilizar 
\textit{GitHub} como fuente de datos, ya que es el sistema de control de versiones por excelencia donde se encuentran 
la mayoría de los proyectos de software de código abierto publicados. De esta manera, tenemos acceso a un repositorio 
de un nivel inferior, ya que no pertenece a un gestor de paquetes específico de un lenguaje de programación, sino que 
se basa su relevancia principalmente en un ámbito más cercano al desarrollo.
Además, la biblioteca ofrece otras funcionalidades, como la obtención en tiempo de ejecución de una red de dependencias 
transitiva para un paquete en particular. También proporciona persistencia de datos en forma de objetos serializados y 
la capacidad de exportar datos en formato \textit{CSV} compatible con \textit{OLIVIA}.

Una vez concluida esta etapa, procedimos a publicar los conjuntos de datos en \textit{Zenodo}\cite{daniel_alonso_bascones_2023_8095863}, con el fin de hacerlos 
accesibles para la comunidad científica. Además, dedicamos nuestros esfuerzos a mejorar la documentación del código 
fuente y generar una documentación completa de la biblioteca. Esta documentación está diseñada para ser accesible desde
 la web y se encuentra alojada en \textit{GitHub Pages}\cite{olivia_finder_docpages}.
La publicación de los conjuntos de datos en \textit{Zenodo} permite a otros investigadores y desarrolladores acceder a 
los datos recopilados y utilizarlos en sus propios proyectos o investigaciones. De esta manera, promovemos la transparencia 
y el intercambio de información entre la comunidad científica.
En cuanto a la documentación de la biblioteca, nos esforzamos por ofrecer una guía clara y concisa sobre cómo utilizar 
la biblioteca, qué funcionalidades y características ofrece, así como ejemplos de uso. La documentación se ha estructurado 
de manera que sea fácil de navegar y buscar información relevante. Al alojarla en \textit{GitHub Pages}, proporcionamos 
un acceso práctico y amigable para los usuarios, quienes pueden acceder a la documentación directamente desde el sitio web
 del proyecto en \textit{GitHub}.
Este \textit{sprint} ha sido difícil de calcular el tiempo que ha llevado realizarlo, debido a que ha habido saltos 
hacia \textit{sprints} anteriores cuando estamos trabajando en este. Ha sido uno de los más complejos. Se calcula un 
periodo de 60 días, 4 reuniones y aproximadamente 150 horas de trabajo.

\subsection{Sprint 4 - Análisis de datos}

En la etapa final de este estudio, se utilizaron los datos recopilados para realizar un análisis con una perspectiva 
evolutiva de las redes de dependencias presentes en los repositorios \textit{CRAN}, \textit{Bioconductor}, \textit{PyPI} 
y \textit{npm}, utilizando la ciencia de redes como marco de referencia.

En este análisis, se lleva a cabo una comparativa entre el estado de la red según los datos obtenidos de \textit{Libraries.io} 
y los datos obtenidos mediante \textit{web scraping}. Se realiza un análisis desde el punto de vista de la centralidad de 
grado y el algoritmo \textit{PageRank}, con el objetivo de identificar y argumentar cuáles son los paquetes más vulnerables 
y por qué. Además, se calculan algunas de las métricas proporcionadas por el modelo de red de \textit{OLIVIA} y se establecen 
relaciones entre ellas.

Este análisis ofrece una perspectiva a nivel micro al examinar los extremos de la red, como los paquetes más destacados, y 
una visión más macro al analizar distribuciones y propiedades más generales, como el grado medio. Su objetivo principal es 
proporcionar una comprensión más profunda de la composición de estas redes de dependencias, sin abordar un análisis exhaustivo 
de toda la red.

Este \textit{sprint} en cuanto a duración fue el más corto y de una duración de 15 días, en los que hubo 3 reuniones, y 
aproximadamente ocupó unas 30 horas de trabajo.


\section{Estudio de viabilidad}

Considerando la dirección y metas de las actividades planificadas, procederemos a evaluar la factibilidad de las mismas, 
teniendo en cuenta su enfoque como un proyecto de investigación, desarrollo e innovación.

\subsection{Viabilidad económica}

\subsubsection{Presupuesto}

Para realizar una estimación precisa del presupuesto del proyecto, hemos considerado diferentes aspectos relacionados 
con los costos involucrados. En primer lugar, hemos establecido una tarifa base de \textit{15€} por hora de trabajo 
para el autor del proyecto, incluyendo los costos salariales, complementos y seguridad social. Cabe destacar que no 
se han identificado gastos significativos en material o hardware, a excepción del consumo eléctrico durante la recolección 
de datos.

En cuanto al software utilizado, nos complace informar que todos los productos empleados en el desarrollo del proyecto 
han sido de coste cero. Sin embargo, es importante mencionar que existe la posibilidad de incurrir en gastos adicionales 
si se decide utilizar servicios de computación en la nube de pago, aunque estos no son estrictamente necesarios para el
 proyecto.

En términos de \textit{costos indirectos}, los cuales abarcan los gastos generales imputables al proyecto, los hemos 
incluido en la partida correspondiente. Siguiendo una práctica común, hemos asignado un \textit{15\%} del presupuesto 
de personal directo como gastos indirectos. De esta manera, hemos obtenido una aproximación al presupuesto total del 
proyecto de $415 \text{ horas} \times 15\textit{€}/\text{hora} \times 1.15 = 7158.75\textit{€}$. Es importante señalar que esta cifra 
no contempla posibles gastos derivados de la publicación y difusión de los resultados, los cuales podrían añadirse en 
etapas posteriores.

En cuanto al salario del profesor, quien cuenta con una mayor experiencia y conocimientos especializados, se ha 
establecido una tarifa base de \textit{30€} por hora. Considerando que ha habido un total de 14 reuniones, cada 
una con una duración aproximada de \textit{1.5} horas, y aplicando los mismos gastos indirectos del \textit{15\%}, 
el costo total asociado a estas reuniones sería de $14 \text{ reuniones} \times 1.5  \text{ horas} \times 30\textit{€}/\text{hora} \times 1.15 = 724\textit{€}$.

En resumen, según nuestras estimaciones, el desarrollo de este proyecto conllevaría un costo aproximado de \textit{9283.5€}. 
Este presupuesto abarca los salarios del autor del trabajo y del profesor, los gastos indirectos correspondientes y otros 
aspectos operativos relevantes.

Por último, es importante mencionar que, debido a la naturaleza del proyecto como una iniciativa de código abierto e 
investigación, no se permite la venta ni la comercialización del producto resultante con fines comerciales. El objetivo 
principal es promover la colaboración y el intercambio de conocimientos en la comunidad científica.

\subsubsection{Financiación}

Los programas nacionales o europeos de financiación destinados a la \textit{investigación, desarrollo e innovación (I+D+i)} 
podrían constituir una opción viable para sufragar total o parcialmente el proyecto. Por lo general, resulta necesario contar 
con una \textit{afiliación} a un marco empresarial o académico específico que permita acceder a los 
diversos \textit{instrumentos de financiación} disponibles.

Una interesante posibilidad que merece ser estudiada, y que ha cobrado relevancia en el contexto del movimiento 
de \textit{Ciencia Abierta}, es el \textit{crowdfunding}. Esta modalidad persigue la obtención de dinero  
para el proyecto mediante contribuciones económicas realizadas por personas interesadas en contribuir al progreso 
de la \textit{investigación científica transparente} y con \textit{resultados públicos, accesibles y gratuitos}. Algunos ejemplos son
\textit{Kickstarter}\cite{kickstarter} o \textit{Indiegogo}\cite{indiegogo}.

En el ámbito del \textit{crowdfunding}, es de una \textit{importancia capital} presentar de forma atractiva 
la idea del proyecto, destacando su \textit{relevancia e impacto} en la \textit{comunidad científica} y en 
la sociedad en general. Asimismo, es fundamental establecer una estrategia de comunicación efectiva 
para alcanzar a potenciales colaboradores y motivarlos a realizar sus aportaciones económicas.


\subsection{Viabilidad legal}

La viabilidad legal del proyecto software que hemos desarrollado es favorable, considerando las siguientes características:

\textbf{Naturaleza \textit{opensource} y gratuita:} 

Todo el proyecto ha sido diseñado bajo una filosofía de código abierto y gratuito, 
lo que implica que el software y sus componentes están disponibles para ser utilizados, modificados y distribuidos sin 
restricciones. Esta elección nos permite fomentar la colaboración y el acceso abierto a la tecnología desarrollada.

La biblioteca que hemos desarrollado para el proyecto tiene 
una naturaleza experimental y no comercial. Esto implica que su propósito es explorar nuevas ideas y soluciones tecnológicas, 
sin intención de generar beneficios económicos directos a través de su venta o licenciamiento.

\textbf{Recursos extraídos de la red:}

Los recursos utilizados en el proyecto, como bibliotecas y herramientas, han sido obtenidos 
de fuentes abiertas y accesibles en Internet. Al utilizar estos recursos, hemos respetado las condiciones y términos de 
uso establecidos por los autores y las licencias correspondientes.

El conjunto de datos utilizado en el proyecto ha sido recolectado de fuentes 
accesibles en la red, donde cualquier persona puede acceder a ellos. Nos hemos esforzado en recopilar y proporcionar estos 
datos a la comunidad, siguiendo los principios de transparencia y acceso abierto. En el caso de uso de estos datos, sólo 
exigimos la atribución a los autores del Trabajo Fin de Grado\footnote{Daniel Alonso Báscones y Carlos López Nozal}.

Aprovechamos este momento para dar atribución al autor del conjunto de datos de \textit{Libraries.io} (\textit{Tidelift}\footnote{\url{https://tidelift.com/}}).

\textbf{Licencia de código abierto:}

Con respecto a la clasificación de la licencia adecuada para el proyecto, considerando los aspectos mencionados, una opción 
apropiada podría ser la licencia \textit{Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)}\cite{enwiki:1161291260}. Esta licencia 
permite utilizar, modificar y distribuir el software y sus componentes, siempre y cuando se atribuya adecuadamente a los 
autores originales y se comparta bajo una licencia similar. Esta elección refuerza los principios de acceso abierto y 
fomenta la colaboración en la comunidad de desarrolladores y usuarios del software.

Otra licencia que se adapta bien a nuestro caso es la licencia \textit{MIT}\cite{enwiki:1152945138}, también reconocida como 
\textit{Licencia Expat MIT}, se destaca como una licencia de software de código abierto ampliamente aceptada y 
prevalente en el ámbito tecnológico. Esta licencia fue concebida por el \textit{Instituto Tecnológico de Massachusetts
(MIT)} y se cataloga como una licencia permisiva.

\textit{La licencia MIT} concede a los desarrolladores de software la capacidad de utilizar, modificar, distribuir 
y sublicenciar el software sin restricciones significativas. Su enfoque adopta una postura de \textit{haz lo que desees}, 
brindando a los usuarios una libertad amplia para emplear el software tanto para propósitos comerciales como no comerciales.

La característica sobresaliente de \textit{la licencia MIT} radica en su concisión y simplicidad. El texto de la 
licencia establece claramente que se otorga permiso para utilizar el software \textit{tal como está}, sin otorgar 
garantías de ningún tipo. Además, se exige la inclusión del aviso de copyright y la exención de 
responsabilidad en todas las copias del software.

Es relevante destacar que la licencia MIT se ajusta de manera óptima a nuestro caso de investigación, dado 
que su naturaleza permisiva y su enfoque en la libertad del usuario se alinean con los objetivos y requisitos de 
nuestro proyecto. 

En base a estas dos licencias y las características de cada una, \texttt{hemos decidido utilizar la licencia MIT}, confiando en su idoneidad para respaldar nuestras actividades de código abierto.


